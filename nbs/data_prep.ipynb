{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, combine and delete original csv's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of file names\n",
    "files = [\n",
    "    \"barcelona_weekdays.csv\", \"budapest_weekends.csv\", \"london_weekends.csv\", \"vienna_weekdays.csv\",\n",
    "    \"amsterdam_weekdays.csv\", \"barcelona_weekends.csv\", \"paris_weekdays.csv\", \"vienna_weekends.csv\",\n",
    "    \"amsterdam_weekends.csv\", \"berlin_weekdays.csv\", \"lisbon_weekdays.csv\", \"paris_weekends.csv\",\n",
    "    \"athens_weekdays.csv\", \"berlin_weekends.csv\", \"lisbon_weekends.csv\", \"rome_weekdays.csv\",\n",
    "    \"athens_weekends.csv\", \"budapest_weekdays.csv\", \"london_weekdays.csv\", \"rome_weekends.csv\"\n",
    "]\n",
    "\n",
    "# Directory containing files\n",
    "directory = \"../data/\"\n",
    "\n",
    "# Initialize an empty list to hold dataframes\n",
    "dataframes = []\n",
    "\n",
    "# Read each file and append to the list\n",
    "for file in files:\n",
    "    # Create the full path to the file\n",
    "    file_path = os.path.join(directory, file)\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Add a column to identify the file (city and weekday/weekend)\n",
    "    city, period = file.replace('.csv', '').rsplit('_', 1)\n",
    "    df['city'] = city.capitalize()\n",
    "    df['period'] = period.capitalize()\n",
    "    # Append the dataframe to the list\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Delete all the original files\n",
    "for file in files:\n",
    "    os.remove(os.path.join(directory, file))  # Delete each file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add GDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GDP from wiki - United Nations estimate\n",
    "# https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)_per_capita\n",
    "gdp_mapping = {\n",
    "'Barcelona' : 30.058,       \n",
    "'Budapest'  : 18.728,   \n",
    "'London'    : 46.542,   \n",
    "'Vienna'    : 53.840,   \n",
    "'Amsterdam' : 57.871,       \n",
    "'Paris'     : 44.229,   \n",
    "'Berlin'    : 51.073,   \n",
    "'Lisbon'    : 24.651,   \n",
    "'Athens'    : 20.571,   \n",
    "'Rome'      : 37.150}\n",
    "\n",
    "# Apply the mapping to create a new 'GDP' column\n",
    "combined_df['GDP'] = combined_df['city'].map(gdp_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove very expensive rentals (1500 EUR pr 2 nights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258 rows and 0.005% of the data removed\n"
     ]
    }
   ],
   "source": [
    "# filter out price over 1500\n",
    "data = combined_df[combined_df['realSum'] < 1500]\n",
    "\n",
    "# print message\n",
    "removed = combined_df.shape[0]-data.shape[0]\n",
    "percent = removed/combined_df.shape[0]\n",
    "print(f'{removed} rows and {round(percent,4)}% of the data removed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['realSum', 'room_type', 'person_capacity', 'biz', 'bedrooms', 'dist', 'metro_dist', 'city', 'period', 'attr_index', 'GDP']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../data/clean_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['room_type', 'person_capacity', 'biz', 'bedrooms', 'dist', 'metro_dist', 'city', 'period', 'attr_index', 'GDP']]\n",
    "y = data[['realSum']]\n",
    "\n",
    "# 15% saved for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.15, \n",
    "                                                    random_state=42)\n",
    "\n",
    "# 15% of the remaining saved for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, \n",
    "                                                  y_train,\n",
    "                                                  test_size=X_test.shape[0] / X_train.shape[0],  \n",
    "                                                  random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(df, categorical_cols):\n",
    "    return pd.get_dummies(df, columns=categorical_cols)\n",
    "\n",
    "def normalize(df, numerical_cols, scaler=None):\n",
    "    if not scaler:\n",
    "        scaler = MinMaxScaler()\n",
    "        df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "    else:\n",
    "        df[numerical_cols] = scaler.transform(df[numerical_cols])\n",
    "    return df, scaler\n",
    "\n",
    "def preprocess_data(X_train, X_val, X_test, categorical_cols, numerical_cols, y_train, y_val, y_test):\n",
    "    # Original data (no preprocessing)\n",
    "    original = (X_train.copy(), X_val.copy(), X_test.copy(), y_train.copy(), y_val.copy(), y_test.copy())\n",
    "    \n",
    "    # One-hot encoded data\n",
    "    X_train_hot = one_hot_encode(X_train.copy(), categorical_cols)\n",
    "    X_val_hot = one_hot_encode(X_val.copy(), categorical_cols)\n",
    "    X_test_hot = one_hot_encode(X_test.copy(), categorical_cols)\n",
    "    \n",
    "    hot = (X_train_hot, X_val_hot, X_test_hot, y_train.copy(), y_val.copy(), y_test.copy())\n",
    "    \n",
    "    # Normalized data (excluding categorical columns)\n",
    "    X_train_norm, scaler_X = normalize(X_train.copy(), numerical_cols)\n",
    "    X_val_norm, _ = normalize(X_val.copy(), numerical_cols, scaler=scaler_X)\n",
    "    X_test_norm, _ = normalize(X_test.copy(), numerical_cols, scaler=scaler_X)\n",
    "    \n",
    "    y_train_norm, scaler_y = normalize(y_train.copy(), ['realSum'])\n",
    "    y_val_norm, _ = normalize(y_val.copy(), ['realSum'], scaler=scaler_y)\n",
    "    y_test_norm, _ = normalize(y_test.copy(), ['realSum'], scaler=scaler_y)\n",
    "    \n",
    "    norm = (X_train_norm, X_val_norm, X_test_norm, y_train_norm, y_val_norm, y_test_norm)\n",
    "    \n",
    "    # One hot and normalised\n",
    "    X_train_hot_norm = one_hot_encode(X_train_norm.copy(), categorical_cols)\n",
    "    X_val_hot_norm = one_hot_encode(X_val_norm.copy(), categorical_cols)\n",
    "    X_test_hot_norm = one_hot_encode(X_test_norm.copy(), categorical_cols)\n",
    "\n",
    "    # Collect the final datasets\n",
    "    hot_norm = (X_train_hot_norm, X_val_hot_norm, X_test_hot_norm, y_train_norm, y_val_norm, y_test_norm)\n",
    "    \n",
    "    return original, hot, norm, hot_norm, scaler_X, scaler_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure directories exist\n",
    "os.makedirs('../data/original', exist_ok=True)\n",
    "os.makedirs('../data/hot', exist_ok=True)\n",
    "os.makedirs('../data/norm', exist_ok=True)\n",
    "os.makedirs('../data/hot_norm', exist_ok=True)\n",
    "\n",
    "def save_datasets(dataset, folder_name):\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = dataset\n",
    "    with open(f'../data/{folder_name}/train.pkl', 'wb') as f:\n",
    "        pickle.dump((X_train.to_numpy(), y_train.to_numpy()), f)\n",
    "    with open(f'../data/{folder_name}/val.pkl', 'wb') as f:\n",
    "        pickle.dump((X_val.to_numpy(), y_val.to_numpy()), f)\n",
    "    with open(f'../data/{folder_name}/test.pkl', 'wb') as f:\n",
    "        pickle.dump((X_test.to_numpy(), y_test.to_numpy()), f)\n",
    "\n",
    "original, hot, norm, hot_norm, scaler_X, scaler_y = preprocess_data(X_train, X_val, X_test, \n",
    "                                                                    categorical_cols=['room_type', 'city', 'period'], \n",
    "                                                                    numerical_cols=['person_capacity', 'biz', 'bedrooms', 'dist', 'metro_dist', 'attr_index', 'GDP'], \n",
    "                                                                    y_train=y_train, y_val=y_val, y_test=y_test)\n",
    "\n",
    "# Save datasets\n",
    "save_datasets(original, 'original')\n",
    "save_datasets(hot, 'hot')\n",
    "save_datasets(norm, 'norm')\n",
    "save_datasets(hot_norm, 'hot_norm')\n",
    "\n",
    "# Save scalers\n",
    "with open('../data/norm/scaler_X.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_X, f)\n",
    "with open('../data/norm/scaler_y.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_y, f)\n",
    "# Save scalers\n",
    "with open('../data/hot_norm/scaler_X.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_X, f)\n",
    "with open('../data/hot_norm/scaler_y.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_y, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airbnb",
   "language": "python",
   "name": "airbnb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
