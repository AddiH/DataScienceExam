{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, combine and delete original csv's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of file names\n",
    "files = [\n",
    "    \"barcelona_weekdays.csv\", \"budapest_weekends.csv\", \"london_weekends.csv\", \"vienna_weekdays.csv\",\n",
    "    \"amsterdam_weekdays.csv\", \"barcelona_weekends.csv\", \"paris_weekdays.csv\", \"vienna_weekends.csv\",\n",
    "    \"amsterdam_weekends.csv\", \"berlin_weekdays.csv\", \"lisbon_weekdays.csv\", \"paris_weekends.csv\",\n",
    "    \"athens_weekdays.csv\", \"berlin_weekends.csv\", \"lisbon_weekends.csv\", \"rome_weekdays.csv\",\n",
    "    \"athens_weekends.csv\", \"budapest_weekdays.csv\", \"london_weekdays.csv\", \"rome_weekends.csv\"\n",
    "]\n",
    "\n",
    "# Directory containing files\n",
    "directory = \"../data/\"\n",
    "\n",
    "# Initialize an empty list to hold dataframes\n",
    "dataframes = []\n",
    "\n",
    "# Read each file and append to the list\n",
    "for file in files:\n",
    "    # Create the full path to the file\n",
    "    file_path = os.path.join(directory, file)\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Add a column to identify the file (city and weekday/weekend)\n",
    "    city, period = file.replace('.csv', '').rsplit('_', 1)\n",
    "    df['city'] = city.capitalize()\n",
    "    df['period'] = period.capitalize()\n",
    "    # Append the dataframe to the list\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Delete all the original files\n",
    "for file in files:\n",
    "    os.remove(os.path.join(directory, file))  # Delete each file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove very expensive rentals (1500 EUR pr 2 nights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258 rows and 0.005% of the data removed\n"
     ]
    }
   ],
   "source": [
    "# filter out price over 1500\n",
    "data = combined_df[combined_df['realSum'] < 1500]\n",
    "\n",
    "# print message\n",
    "removed = combined_df.shape[0]-data.shape[0]\n",
    "percent = removed/combined_df.shape[0]\n",
    "print(f'{removed} rows and {round(percent,4)}% of the data removed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keep only relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['realSum', 'room_type', 'person_capacity', 'biz', 'multi', 'bedrooms', 'dist', 'metro_dist', 'city', 'period', 'attr_index_norm', 'rest_index_norm']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make \"single\" variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14592/2996774527.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['single'] = np.where((data['biz'] == 0) & (data['multi'] == 0), 1, 0)\n"
     ]
    }
   ],
   "source": [
    "# make new column called 'single' when both 'biz' and 'multi' are 0\n",
    "data['single'] = np.where((data['biz'] == 0) & (data['multi'] == 0), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['single'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../data/clean_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "data = pd.read_csv('../data/clean_data.csv')\n",
    "\n",
    "hot_cols = ['room_type', 'city', 'period'] # Columns to one-hot encode\n",
    "\n",
    "norm_cols = ['person_capacity', # Columns to normalize\n",
    "            'bedrooms',\n",
    "            'dist',\n",
    "            'metro_dist',\n",
    "            'attr_index',\n",
    "            'rest_index',\n",
    "            'GDP'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[['realSum']]\n",
    "X = data.drop(columns='realSum')\n",
    "\n",
    "# 15% saved for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.15, \n",
    "                                                    random_state=42)\n",
    "\n",
    "# 15% of the remaining saved for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, \n",
    "                                                  y_train,\n",
    "                                                  test_size=X_test.shape[0] / X_train.shape[0],  \n",
    "                                                  random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(df, categorical_cols):\n",
    "    return pd.get_dummies(df, columns=categorical_cols, dtype=int)\n",
    "\n",
    "def normalize(df, numerical_cols, scaler=None):\n",
    "    if not scaler:\n",
    "        scaler = MinMaxScaler()\n",
    "        df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "    else:\n",
    "        df[numerical_cols] = scaler.transform(df[numerical_cols])\n",
    "    return df, scaler\n",
    "\n",
    "def preprocess_data(X_train, X_val, X_test, categorical_cols, numerical_cols, y_train, y_val, y_test):\n",
    "    # One-hot encoded data\n",
    "    X_train_hot = one_hot_encode(X_train.copy(), categorical_cols)\n",
    "    X_val_hot = one_hot_encode(X_val.copy(), categorical_cols)\n",
    "    X_test_hot = one_hot_encode(X_test.copy(), categorical_cols)\n",
    "    \n",
    "    # delete the column ['period_Weekdays']\n",
    "    X_train_hot = X_train_hot.drop(columns=['period_Weekdays'])\n",
    "    X_val_hot = X_val_hot.drop(columns=['period_Weekdays'])\n",
    "    X_test_hot = X_test_hot.drop(columns=['period_Weekdays'])\n",
    "\n",
    "    \n",
    "    hot = (X_train_hot, X_val_hot, X_test_hot, y_train.copy(), y_val.copy(), y_test.copy())\n",
    "    \n",
    "    # Normalized data (excluding categorical columns)\n",
    "    scaler_X = MinMaxScaler()\n",
    "    X_train_hot_norm = scaler_X.fit_transform(X_train_hot.copy())\n",
    "    X_val_hot_norm = scaler_X.transform(X_val_hot.copy())\n",
    "    X_test_hot_norm = scaler_X.transform(X_test_hot.copy())\n",
    "\n",
    "    scaler_y = MinMaxScaler()\n",
    "    y_train_hot_norm = scaler_y.fit_transform(y_train.copy())\n",
    "    y_val_hot_norm = scaler_y.transform(y_val.copy())\n",
    "    y_test_hot_norm = scaler_y.transform(y_test.copy())\n",
    "    \n",
    "    #X_train_hot_norm, scaler_X = normalize(X_train_hot.copy(), numerical_cols)\n",
    "    #X_val_hot_norm, _ = normalize(X_val_hot.copy(), numerical_cols, scaler=scaler_X)\n",
    "    #X_test_hot_norm, _ = normalize(X_test_hot.copy(), numerical_cols, scaler=scaler_X)\n",
    "    \n",
    "    #y_train_hot_norm, scaler_y = normalize(y_train.copy(), ['realSum'])\n",
    "    #y_val_hot_norm, _ = normalize(y_val.copy(), ['realSum'], scaler=scaler_y)\n",
    "    #y_test_hot_norm, _ = normalize(y_test.copy(), ['realSum'], scaler=scaler_y)\n",
    "    \n",
    "    # Collect the final datasets\n",
    "    hot_norm = (X_train_hot_norm, X_val_hot_norm, X_test_hot_norm, y_train_hot_norm, y_val_hot_norm, y_test_hot_norm)\n",
    "    \n",
    "    return hot, hot_norm, scaler_X, scaler_y\n",
    "\n",
    "#def save_datasets(dataset, folder_name):\n",
    "#    X_train, X_val, X_test, y_train, y_val, y_test = dataset\n",
    "#    with open(f'../data/{folder_name}/train.pkl', 'wb') as f:\n",
    "#        pickle.dump((X_train.to_numpy(), y_train.to_numpy()), f)\n",
    "#    with open(f'../data/{folder_name}/val.pkl', 'wb') as f:\n",
    "#        pickle.dump((X_val.to_numpy(), y_val.to_numpy()), f)\n",
    "#    with open(f'../data/{folder_name}/test.pkl', 'wb') as f:\n",
    "#        pickle.dump((X_test.to_numpy(), y_test.to_numpy()), f)\n",
    "\n",
    "def save_datasets(dataset, folder_name):\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = dataset\n",
    "    with open(f'../data/{folder_name}/train.pkl', 'wb') as f:\n",
    "        pickle.dump((X_train, y_train), f)\n",
    "    with open(f'../data/{folder_name}/val.pkl', 'wb') as f:\n",
    "        pickle.dump((X_val, y_val), f)\n",
    "    with open(f'../data/{folder_name}/test.pkl', 'wb') as f:\n",
    "        pickle.dump((X_test, y_test), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure directories exist\n",
    "os.makedirs('../data/hot', exist_ok=True)\n",
    "os.makedirs('../data/hot_norm', exist_ok=True)\n",
    "\n",
    "# Preprocess data\n",
    "hot, hot_norm, scaler_X, scaler_y = preprocess_data(X_train=X_train, X_val=X_val, X_test=X_test,\n",
    "                                                    y_train=y_train, y_val=y_val, y_test=y_test ,\n",
    "                                                    categorical_cols=hot_cols, \n",
    "                                                    numerical_cols=norm_cols)\n",
    "\n",
    "# Save datasets\n",
    "save_datasets(hot, 'hot')\n",
    "save_datasets(hot_norm, 'hot_norm')\n",
    "\n",
    "# Save scalers\n",
    "with open('../data/scaler_X.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_X, f)\n",
    "with open('../data/scaler_y.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_y, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airbnb",
   "language": "python",
   "name": "airbnb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
