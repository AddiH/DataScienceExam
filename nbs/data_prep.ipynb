{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, combine and delete original csv's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of file names\n",
    "files = [\n",
    "    \"barcelona_weekdays.csv\", \"budapest_weekends.csv\", \"london_weekends.csv\", \"vienna_weekdays.csv\",\n",
    "    \"amsterdam_weekdays.csv\", \"barcelona_weekends.csv\", \"paris_weekdays.csv\", \"vienna_weekends.csv\",\n",
    "    \"amsterdam_weekends.csv\", \"berlin_weekdays.csv\", \"lisbon_weekdays.csv\", \"paris_weekends.csv\",\n",
    "    \"athens_weekdays.csv\", \"berlin_weekends.csv\", \"lisbon_weekends.csv\", \"rome_weekdays.csv\",\n",
    "    \"athens_weekends.csv\", \"budapest_weekdays.csv\", \"london_weekdays.csv\", \"rome_weekends.csv\"\n",
    "]\n",
    "\n",
    "# Directory containing files\n",
    "directory = \"../data/\"\n",
    "\n",
    "# Initialize an empty list to hold dataframes\n",
    "dataframes = []\n",
    "\n",
    "# Read each file and append to the list\n",
    "for file in files:\n",
    "    # Create the full path to the file\n",
    "    file_path = os.path.join(directory, file)\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Add a column to identify the file (city and weekday/weekend)\n",
    "    city, period = file.replace('.csv', '').rsplit('_', 1)\n",
    "    df['city'] = city.capitalize()\n",
    "    df['period'] = period.capitalize()\n",
    "    # Append the dataframe to the list\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Delete all the original files\n",
    "for file in files:\n",
    "    os.remove(os.path.join(directory, file))  # Delete each file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add GDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GDP from wiki - United Nations estimate\n",
    "# https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)_per_capita\n",
    "gdp_mapping = {\n",
    "'Barcelona' : 30.058,       \n",
    "'Budapest'  : 18.728,   \n",
    "'London'    : 46.542,   \n",
    "'Vienna'    : 53.840,   \n",
    "'Amsterdam' : 57.871,       \n",
    "'Paris'     : 44.229,   \n",
    "'Berlin'    : 51.073,   \n",
    "'Lisbon'    : 24.651,   \n",
    "'Athens'    : 20.571,   \n",
    "'Rome'      : 37.150}\n",
    "\n",
    "# Apply the mapping to create a new 'GDP' column\n",
    "combined_df['GDP'] = combined_df['city'].map(gdp_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove very expensive rentals (1500 EUR pr 2 nights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258 rows and 0.005% of the data removed\n"
     ]
    }
   ],
   "source": [
    "# filter out price over 1500\n",
    "data = combined_df[combined_df['realSum'] < 1500]\n",
    "\n",
    "# print message\n",
    "removed = combined_df.shape[0]-data.shape[0]\n",
    "percent = removed/combined_df.shape[0]\n",
    "print(f'{removed} rows and {round(percent,4)}% of the data removed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['realSum', 'room_type', 'person_capacity', 'biz', 'bedrooms', 'dist', 'metro_dist', 'city', 'period', 'attr_index', 'GDP']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../data/clean_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['room_type', 'person_capacity', 'biz', 'bedrooms', 'dist', 'metro_dist', 'city', 'period', 'attr_index', 'GDP']]\n",
    "y = data[['realSum']]\n",
    "\n",
    "# 15% saved for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.15, \n",
    "                                                    random_state=42)\n",
    "\n",
    "# 15% of the remaining saved for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, \n",
    "                                                  y_train,\n",
    "                                                  test_size=X_test.shape[0] / X_train.shape[0],  \n",
    "                                                  random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save datasets\n",
    "with open('../data/original/train.pkl', 'wb') as f:\n",
    "    pickle.dump((X_train.to_numpy(), y_train.to_numpy()), f)\n",
    "with open('../data/original/val.pkl', 'wb') as f:\n",
    "    pickle.dump((X_val.to_numpy(), y_val.to_numpy()), f)\n",
    "with open('../data/original/test.pkl', 'wb') as f:\n",
    "    pickle.dump((X_test.to_numpy(), y_test.to_numpy()), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Entire home/apt' 'Private room' 'Shared room']\n",
      "['Lisbon' 'Athens' 'Rome' 'Paris' 'Budapest' 'Barcelona' 'London'\n",
      " 'Amsterdam' 'Berlin' 'Vienna']\n",
      "['Weekends' 'Weekdays']\n"
     ]
    }
   ],
   "source": [
    "# Look at factor variables\n",
    "print(X_train['room_type'].unique())\n",
    "print(X_train['city'].unique())\n",
    "print(X_train['period'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(df):\n",
    "    # Apply one-hot encoding to the categorical columns\n",
    "    df_encoded = pd.get_dummies(df, columns=['room_type', 'city', 'period'])\n",
    "    \n",
    "    # Drop one of the period columns to avoid perfect multicollinearity\n",
    "    if 'period_Weekdays' in df_encoded.columns:\n",
    "        df_encoded.drop('period_Weekdays', axis=1, inplace=True)\n",
    "\n",
    "    # Convert all Boolean columns to integers (0 and 1)\n",
    "    for col in df_encoded.columns:\n",
    "        if df_encoded[col].dtype == bool:\n",
    "            df_encoded[col] = df_encoded[col].astype(int)\n",
    "\n",
    "    return df_encoded.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_hot = one_hot(X_train)\n",
    "X_val_hot = one_hot(X_val)\n",
    "X_test_hot = one_hot(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save datasets\n",
    "with open('../data/hot/train.pkl', 'wb') as f:\n",
    "    pickle.dump((X_train_hot, y_train.to_numpy()), f)\n",
    "with open('../data/hot/val.pkl', 'wb') as f:\n",
    "    pickle.dump((X_val_hot, y_val.to_numpy()), f)\n",
    "with open('../data/hot/test.pkl', 'wb') as f:\n",
    "    pickle.dump((X_test_hot, y_test.to_numpy()), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalise AND one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform\n",
    "scaler_x = MinMaxScaler()\n",
    "X_train_hot_norm = scaler_x.fit_transform(X_train_hot)\n",
    "\n",
    "# Only transform\n",
    "X_val_hot_norm = scaler_x.transform(X_val_hot)\n",
    "X_test_hot_norm = scaler_x.transform(X_test_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform\n",
    "scaler_y = MinMaxScaler()\n",
    "y_train_norm = scaler_y.fit_transform(y_train)\n",
    "\n",
    "# Only transform\n",
    "y_val_norm = scaler_x.transform(y_val)\n",
    "y_test_norm = scaler_x.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save datasets\n",
    "with open('../data/hot_norm/train.pkl', 'wb') as f:\n",
    "    pickle.dump((X_train_hot_norm, y_train_norm), f)\n",
    "with open('../data/hot_norm/val.pkl', 'wb') as f:\n",
    "    pickle.dump((X_val_hot_norm, y_val_norm), f)\n",
    "with open('../data/hot_norm/test.pkl', 'wb') as f:\n",
    "    pickle.dump((X_test_hot_norm, y_test_norm), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airbnb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
